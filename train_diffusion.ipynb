{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"data/dataset/captions.jsonl\")\n",
    "dataset = dataset.map(lambda x: {\"file_path\": f\"data/dataset/{x['file_path']}\"})\n",
    "\n",
    "\n",
    "# Preview the dataset\n",
    "print(dataset[\"train\"][0])  # Example: {'file_path': 'images/image1.png', 'caption': 'A description'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel, StableDiffusionPipeline, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Device setup\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load pre-trained Stable Diffusion components\n",
    "model_name = \"CompVis/stable-diffusion-v1-1\"\n",
    "\n",
    "# UNet and scheduler\n",
    "unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\").to(device)\n",
    "scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n",
    "\n",
    "# Load tokenizer and text encoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "\n",
    "# Add a dummy alpha channel to each image\n",
    "def add_alpha_channel(image_tensor):\n",
    "    # Add an extra channel filled with zeros\n",
    "    alpha_channel = torch.zeros_like(image_tensor[:, :1, :, :])  # Shape: [batch_size, 1, H, W]\n",
    "    return torch.cat([image_tensor, alpha_channel], dim=1)  # Shape: [batch_size, 4, H, W]\n",
    "\n",
    "\n",
    "# Correct definition of CustomDataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image_path = item[\"file_path\"]  # Ensure this key exists in your dataset\n",
    "        caption = item[\"caption\"]      # Ensure this key exists in your dataset\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, caption\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Prepare dataset and DataLoader\n",
    "train_dataset = CustomDataset(dataset[\"train\"], transform)\n",
    "data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(list(unet.parameters()) + list(text_encoder.parameters()), lr=5e-5)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for images, captions in data_loader:\n",
    "        with autocast():\n",
    "            images = images.to(device)\n",
    "            images = add_alpha_channel(images)\n",
    "\n",
    "            # Tokenize captions\n",
    "            inputs = tokenizer(captions, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\")\n",
    "            input_ids = inputs.input_ids.to(device)\n",
    "\n",
    "            # Encode text\n",
    "            text_embeddings = text_encoder(input_ids)[0]\n",
    "\n",
    "            # Predict noise (forward pass)\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "            noisy_images = scheduler.add_noise(images, noise, scheduler.timesteps[0])\n",
    "            predicted_noise = unet(noisy_images, scheduler.timesteps[0], text_embeddings).sample\n",
    "\n",
    "            # Compute loss (MSE)\n",
    "            loss = torch.nn.functional.mse_loss(predicted_noise, noise)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed. Loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "unet.save_pretrained(\"fine_tuned_unet\")\n",
    "text_encoder.save_pretrained(\"fine_tuned_text_encoder\")\n",
    "print(\"Fine-tuned model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"stable-diffusion-fine-tuning\")\n",
    "\n",
    "# Log losses during training\n",
    "wandb.log({\"epoch\": epoch, \"loss\": loss.item()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_pipeline = StableDiffusionPipeline.from_pretrained(\"fine_tuned_stable_diffusion\").to(\"cuda\")\n",
    "\n",
    "# Generate an image\n",
    "prompt = \"A beautiful painting of a futuristic cityscape\"\n",
    "image = fine_tuned_pipeline(prompt).images[0]\n",
    "\n",
    "# Save and display the image\n",
    "image.save(\"generated_image.png\")\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load an image and ensure 3 channels (RGB)\n",
    "image = Image.open(\"data/dataset/images/kvg:kanji_0f9a8.png\").convert(\"RGB\")\n",
    "\n",
    "# Transform pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Apply the transform\n",
    "input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "print(input_tensor.shape)  # Output: [1, 3, 128, 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-diffusion-kanji-5un6CDl7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
